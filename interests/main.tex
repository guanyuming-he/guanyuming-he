\documentclass[10pt]{article}

\usepackage{amsmath, amsthm, amssymb}
% See amsthdoc p.~3
\newtheorem*{CT}{Cantor's Theorem}

\usepackage{hyperref}
\usepackage{xurl}

% Only include additional info to compile if I am giving the document
% as a personal statement in my PhD applications.
\includeonly{additional}

\begin{document}
\tableofcontents

\section*{About this document}
This document gives a comprehensive discussion of my current academic
interests. They are categorized into broad fields as sections, and each
subsection, describing my interest for a subfield of it, will generally give my
motivation for studying that field and usually end with a current open problem
that I aim to solve during my research in that subfield.

If you are a professor, I might have sent the whole, a part, or a modified
version of this document to you, as my statement of interest. Generally, you
only need to read the subfields that I want to be working in with you; see the
above table of contents. If you have time, however, I still recommend a linear
read through the document to see the whole picture of my ambition and passion.
Finally, to get the latest version of the document, go to
\url{https://github.com/guanyuming-he/guanyuming-he/blob/main/interests/main.pdf}.

\part{My academic interests}

\section[Security]{Security: Introduction into my interests}
I believe security is a good entrance into my current interests. It awards both
attention to detail and high-level systematic view; its subfields connects
directly with both theoretical and practical areas that fascinates me, and it
offers effective tools to solve the imminent problems I face in real life. This
section will take such a flow that the subfields of my interest will be first
led to from real world problems and motivation, and then be explored with my
more academical and philosophical pursuits.

The term \emph{security} in computer science generally carries the sense of
achieving \emph{security goals} through \emph{mechanisms or properties} of a
system, despite the presence of adversaries in a \emph{threat model}. The
threat model has to be carefully chosen and reviewed; there would be no
security against an omnipotent adversary.

Unlike in math, one cannot arbitrarily wiggle the threat model to one's desire.
On the contrary, one has little control to how one's digital rights are
treated, which are increasingly threatened (e.g. \cite{eu.digital.1,
eu.digital.2, internet.shutdown.2024}). A more prominent example is the
continuous push for exceptional access to people's data and communication in
various forms by governments\footnote{ For a recent instance, EU revived the
\href{
https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM\%3A2022\%3A209\%3AFIN}
{Chat Control} proposal in 2025. See \url{
https://eutechloop.com/time-is-running-chat-control/}.}, ignoring expert
opinions against such measures \cite{keys.under.doormats, bugs.in.our.pockets,
chatcontrolchildprotection}. Prof.~Anderson has coined the fight for the
control of cryptography (thus a large portion of peoples' digital rights)
between governments and all who try to fight back using the term ``(thirty
years of) crypto war'' \cite{anderson.freedom}. In this section, you will see a
similar tone to Prof.~Anderson's honesty which I admire --- I aim to approach
my work with the same honesty --- to confront problems as they are, to fight
against injustice even when inconvenient, and to maintain my integrity
throughout my academic journey.  In fact, Prof.~Anderson's section
\emph{Privacy and freedom issues} \cite{anderson.freedom} on his homepage was
one major inspiration that set me on the path of cybersecurity, but which I
will not elaborate in the document.

Moreover, I unfortunately have lived in a place where
peoples' digital rights are much more aggressively invaded, particularly in the
form of systematic, comprehensive, and far-reaching censorship and surveillance
\cite{internet.coup} \cite[Sect.~5]{chall.censor.circum}.  These threats
present one of the major real world problems I aim to address by my research in
security and my main realistic motivation. Through the document, you will see
my philosophical motivations as well.

\subsection[Secure Systems]{
Secure Systems: Understanding the mechanisms} 
\label{sec.secure.systems}

\subsubsection{Low-level system details} \label{sec.low.level}
Beside the threat model, when we talk about security, we also implicitly assume
another model, environment, or host, that encapsulates the problem. For
instance, in the context of isolation, 
\begin{itemize}
	\item The host that encapsulates process isolation is the operating system
		kernel.
	\item The host that encapsulates virtual machine isolation is the
		hypervisor, often including hardware support.
	\item The host that encapsulates air-gapped machine isolation is the
		physical world, or more abstractly, the physical laws.
\end{itemize} 
These hosts form a hierarchy. For instance, processes inside an OS may run
inside a virtual machine, on hardware, within the physical world.

As a result, a security researcher has to understand these hosts, each to a
different degree, depending on how secure she wants her systems to be. Even if
a system is formally verified to be secure within a host, it can still be
attacked from interactions with an outer host. That is well examplified by
the Meltdown attack, which breaks both kernel and hypervisor's isolation of
memory, because an outer host, the hardware, has a vulnerability
\cite{meltdown}. In fact, Meltdown touches more than logical flaws in hardware;
it relies on the timing difference of cache accesses to extract information
from it \cite{flush.reload}, one that directly connects with the physical
world. At this point, I feel that I should also mention the Spectre attacks
family \cite{spectre}, which, unlike Meltdown, does not exploit a clear logic
bug or fault in the hardware. Instead, it work by observing the side effects
of various implementations of speculative execution, which is arguably not a
bug but simply a feature implemented without caution to how information may be
leaked via the outer hosts.

Indeed, there is little hope to understand and model the universe thoroughly
and accurately in one's life-time, and one can expect endless discoveries of
how an otherwise secure system can leak information in unexpected ways, such as
power and radiation via physical effects, which opens to a large range of
attacks \cite{side.channel.1, side.channel.2, side.channel.3, flush.reload},
collectively defined as side-channel attacks.  Then, one must ask, is it futile
trying to cope with every such possible attack? My opinion is that it depends
on the current understanding of the whole cybersecurity community. One novel
attack just discovered may be increasingly understood and gain popularity by
the whole community and thus gradually become a necessary part every secure
system designer should take into account. I view this as an everlasting
dynamics where people continuously push the boundary of our understanding, and
consequently expanding the requirements for all in the community.

Therefore, a good understanding of the most common and well-understood hosts,
the operating systems and the hardware (including the architecture), will be
essential for me as a security researcher. Additionally, figuring out how to
perform clever hacks (as in the hacking culture of MIT, not the mainstream
meaning of cracking) based on knowledge of details of a system gives a great
sense of achievement to me. Moreover, this continuous expansion of knowledge
frontier carries a special implication in security. Whereas a mathematician is
perfectly fine to specialize in one direction without knowing every details in
other fields, a computer system can never be secure if only designed with one
perspective. For now, a good system is usually designed by collaboration; and
it will be a very interesting challenge to figure out how one single researcher
can be empowered to do this task better; could the LLMs be a potential
assistant who fills the knowledge gaps? Finally, this is one major place where
cybersecurity connects directly with a broad range of other fields, including
software engineering, systems architecture, networking, and physics.  These
together contribute to my interest in the low-level details of secure systems.

\subsubsection{A high-level view of the Internet} \label{sec.high.level}
A high-level understanding of how the digital infrastructures work is essential
to understand what enables the threats to digital rights on top of them. The
current networking infrastructure, in my opinion, has conflicting properties.
On one hand, the fundamental problem of the impossibility to link every two
computers requires sharing of links, a solution that welcomes centralization.
On the other hand, the poor scalability of simple sharing schemes of a link
(usually via a switch) necessitates a better scheme to extend a small network
globally. To achieve this, the Internet relies heavily on delegation and
distributed structures, such as topological divisions of its address space and
delegation of most routing responsibilities to each individual networks (e.g.\
ISPs). Its BGP, opearing in between them, is mainly concerned exchanging
reachability information, whereas its IGPs handle routing paths and allow
different networks to implement different routing policies.

Therefore, the Internet has become a mixture of centralization and
decentralization, where each end node is managed by an ISP network, yet no
single ISP runs the whole Internet. Perhaps surprisingly, I found this hybrid
structure more optimal for localized sabotage at the state level, creating
``sub-Internet''s, each of which is crippled at a different level. 

Unfortunately, the lower layers of the Internet have proven to have a great
inertia for change.  Handley gave a nice discussion on it in 2006
\cite{why.internet.just.works}, and the trend he described has mostly been the
same since then: ``\emph{the core Internet protocols have not changed
significantly in more than a decade, in spite of exponential growth in the
number of Internet users and the speed of the fastest links.}''
\cite{why.internet.just.works}. On the other hand, the protocols in the higher
layers evolved to a much greater degree. As the transport layer welcomes
QUIC\cite{quic}, the application layer has accumulated an enormous amount of
innovation and progress, in particular, onion routing and
Tor\cite{onion.routing, tor}, Bitcoin\cite{bitcoin}, VPN
protocols\cite{openvpn, wireguard} and decentralized instant chat\cite{matrix,
tox}, that fight for digital rights and/or promote decentralization. Yet one
must not overlook the crypto-constructs that lay the foundation for all of
them, which I discuss in detail in Section~\ref{sec.crypto}.

Unfortunately again, because all of these are built on the Internet, a
state-level censor could easily abuse its local authority to target them. A few
regimes are notorious to have blocked a vast amount of them to various
extents, employing complicated passive analysis and active probing techniques
\cite{censor.block.1, censor.block.2,censor.block.3, censor.block.4,
censor.block.5,censor.block.6}.
Although a state would need to consider the collatoral damage, a totalitarian
regime would not hesitate to block an entire protocol before it can figure out
how to block it selectively \cite{selective.block.1, censor.block.4}. 
Apart from state-level actors, because commerical local ISPs additionally
have an incentive in income and profit, not only do they perform surveillance
and censorship like state-actors \cite{isp.statelike.actions.1,
isp.statelike.actions.2}, they also implement unjust policies easily with their
local control of the infrastructure, like unfairly limiting the use of certain
P2P protocols \cite{isp.block.p2p.1, isp.block.p2p.2, isp.block.p2p.3,
isp.statelike.actions.1}.  Although ISPs argue that these P2P protocols can
consume too much bandwidth, the other side of the story is that ISPs often
oversubscribe and fraudulently advertise the bandwidth of Internet service they
provide \cite{isp.oversubscribe.1, isp.oversubscribe.2}. This essentially is a
probabilistic exploit on its customers --- when almost all of the users happen
to use the maximal bandwidth the ISP sells to them, congestion and throttling
occur, and the users, not the ISP, ultimately pay the price --- this is also
the same kind of injustice imposed by airlines who oversell tickets.

The question of how to build protocols and systems that preserve one's rights
on top of the Internet that powerful adversaries control, fighting against
surveillance, censorship, and overall other unfair practices which the
infrastructures of the current networks happen to enable, is central to my
interest in the high-level design of secure distributed systems.


\subsection[Data-driven analysis]{
Data-driven analysis: Lights through artifical clouds}
Despite my intention to understand systems, many of them are not open for
analysis, for various reasons. In particular, the aforementioned systems for
digital surveillance and censorship are not only proprietary but often state
secrets. However, just as no material can perfectly insulate against heat in
physics, no system can perfectly isolate information; some of it inevitably
leaks through side channels (see Section~\ref{sec.low.level}). 

By carefully observing their behaviors and probing their responses under
controlled conditions, we can gain insight. Combined with data science
techniques, these observations reveal hidden structures and enable us to draw
meaningful conclusions.

The process usually involves 1) raising questions and claims about some
properties of a target system 2) conducting experiments (often involving
purchased servers within such as Aliyun) 3) analyzing data 4) answering
questions and verifying claims. As a simple example, Sheffey et al.\ very
recently studied the IP addresses injected by the GFW censorship system. By
first finding injected IPs and then probing them within the GFW, they found
three categories of such IPs \cite{gfw.injected.ip}. 

Thanks to continuous work from both academic scholars and dedicated
organizations, (e.g.\ \cite{data.analysis.1, data.analysis.2, censor.block.6}),
I and a small set of others who suffer from such systems and who are fortunate
enough to know these works, could know what we are facing everyday better.
Conversely, I strongly hope to contribute to the free-side of the arms race,
making the free Internet reachable to everyone.

\subsection[Cryptography]{Cryptography: A mathematical savior}
\label{sec.crypto}
Security is often described as an arms race --- that who controls more
resources tends to discover more vulnerabilities, devise more attacks, and
harden their systems more. Where is hope, then, when most self-censor and give
up fighting the regime \cite{self.censor.1, self.censor.2, self.censor.3}  and
even fewer of the fighters have sufficient skills to join in the security war
\cite{defenders.lack.skills.1, defenders.lack.skills.2}?

I believe one solution to what might sound like a power struggle lies within
(modern) cryptography. For thousands of years since the use of the earliest
symmetric encryption methods like the Ceasar cipher, the ability to securely
communicate through an insecure channel had still been mostly limited to the
privileged who could afford persistent access to physical secure channels to
exchange the keys and reliable safeguarding of the keys.  A stunning turning
point was found in what was widely regarded as the beginning of modern
cryptography, Deffie and Hellman's \emph{New Directions in Cryptography}
\cite{new.directions.crypto}, where they gave a practical mathematical
procedure to Merkle's original idea for establishing a key known only to both
parties over an insecure channel.

At first glance, the idea sounded so impossible that Merkel himself faced 
rejections when presenting the idea to his then professor Hoffman and to the
CACM \cite{merkle.rejection}:
\begin{quotation}
	``I am sorry to have to inform you that the paper is not in the main stream
	of present cryptography thinking and I would not recommend that it be
	published in the Communications of the ACM.''

	``Experience shows that it is extremely dangerous to transmit key
	information in the clear.''\cite{merkle.rejection}
\end{quotation}
The rejections represented a concensus of the old cryptography community that
even Shannon concurred with: ``\emph{The key must be transmitted by
non-interceptible means from transmitting to receiving points}''
\cite[p.~670]{shannon.theory.secrecy}, which demonstrates how ``paradoxical''
the idea was.

Such ``paradoxical'' ideas of modern cryptography are what attract me most to
it, because the more paradoxical such an idea is, the greater extent we know
one can preserve one's rights, even when threatened by extremely powerful
adversaries\footnote{Albeit unfortunately they all rely on some assumptions,
particularly that some problems are hard.}.
Thus, such ideas become the enabler that underlies those systems mentioned in
section~\ref{sec.high.level}: TOR, Bitcoin, Matrix, etc., and the
catalyst that leads to the massive adoption of society advancements such as
e-commerce. Remarkably, in less than 50 years since 1976, we already have a
number of such discoveries in addition, and here's a list of some of them.
\begin{description}
\item[Pseudorandom functions] The kind of deterministic algorithms
	whose outputs look like that of a random oracle, by block ciphers like
	AES\cite{aes} or other ways \cite{pseudo.rand.cons.2}, has a strong link
	with various other essential constructs like one-way functions.

\item[Zero-knowledge proofs] By interactively leveraging challenges that only a
	true prover could easily solve, zero-knowledge proof \cite{zero.knowledge}
	enables one to verify that the prover knows a witness of a problem in NP
	\cite{zero.knowledge.np} without revealing the witness.

\item[Homomorphic encryptions] Doing computation on encrypted data has been a
	very desirable property for many years. Although many earliest public-key
	encryption schemes, such as RSA \cite{rsa} and ElGamal \cite{elgamal},
	already natively supported limited homomorphism like modular
	multiplication by their design, the first full scheme that allowed
	arbitrary computation on encrypted data was only proposed in 2009
	\cite{first.full.homo}.
\end{description}

These discoveries have many applications and implications, two of which I pay
most attention to. One is that they update our understanding of certain
theoretical lower bounds of how much security one can achieve in the face of
strong adversaries, no matter the power difference between them. The other is
how they help minimize the trust required to perform global-level of
collaboration that involves people from vastly different backgrounds and
beliefs; such a collaboration that would typically require a strong central
authority to organize, now only requires the participants to trust a few
foundamental assumptions of cryptography.

Personally, I would go a step further. I believe modern cryptography offers not
only technical tools, but a possible solution for addressing one of the big
challenges our democracies face today: fragmentation, polarization, and the
erosion of shared trust. By design, cryptography redistributes power --- it
makes mass surveillance prohibitively costly, even for state-level actors, by
forcing them to target individuals rather than populations. For example,
suppose breaking one person’s encryption, on average, required a single day of
dedicated effort (through side-channels, vulnerabilities, or social engineering
rather than mathematics), that constraint alone would prevent massive
surveillance, since no adversary has that many days or resources to do this on
everyone. In this way, encryption does something profound: it automatically and
passively unites individuals, protecting each not by active coordination that
is increaingly difficult, but by the collective shield of widespread adoption.
Fortunately, as many modern infrastructures already deploy encryption and other
crypto schemes by default, this ``passive solidarity'' is not a dream but a
reachable reality, one that shows how mathematics can serve as a safeguard for
democracy in an increasingly divided world.

To end this section, cryptography not only flows towards my passion for purer
philosophical understanding, because of its deep connection with theoretical
computer science and mathematics, to which I turn in Section~\ref{sec.math},
but also carries my hope to break free from the strong grasp of the tyrannies
today, one that represents my free will which refuses to lose my agency and
independence, in a world that too often seeks to reduce us into interchangable
screws for the system.  

\section[Software engineering]{
Software engineering: My earliest and continuing passion}
\subsection{Software construction}
The seed for my passion in questioning how things should be built and be amazed
at how complex things interact with each other when assembled together, was
there in me long before I had an idea of what research or the academia were. In
high school, I was obsessed with creating my own video games, what I hoped
would be my sanctuary from my then miserable life. My unsuspectingly ambitious
goal of starting from building a game engine unsurprisingly failed, but it did
introduce me to the book \emph{Game Engine Architecture}
\cite{game.engine.arch}, which isn't academically significant, but its contents
made me realize that I was more into how things work in the engine such as how
threads coordinate with each other than into actually making a game.

This interest of figuring out how a complex system work and how to build such
one, leads me into learning various software engineering principles, in
particular, those of Liskov \cite{liskov.adt, liskov.subtype}, learning them
deeper when offered by my undergraduate courses, and applying some of them in
all of my big projects, since I got to know them. 

With all these principles, engineering of large software systems still remains
a big challenge, particularly in interface design, separation of components,
and all that we still have no fixed rules to follow. In other words, large
systems engineering largely remains an art instead of a science
\cite{no.silver.bullet, prospect.eng.discipline.software}. Could it become a
science one day, governed by well-established rules that guide us toward
optimal software systems in all or at least most situations? That is an
intriguing question I aspire to explore while continuing to construct software
in the following years.

\subsection{Formal methods/verification}
Many software engineering principles are concerned with mistakes made by
humans. Not only do we try to minimize bugs from programs, we also need to
reduce them from the specification or requirements of the software.  Therefore,
formalizing the specification and verifying that the software built indeed
satisfies it could be seen as an ultimate way to eliminate bugs
\cite{hoare.axiomatic.prog}. 

\subsubsection{The speed problem of formalization}
Perhaps unsurprisingly, the industry has a low usage of fully verified software
\cite{formal.methods.underuse}, since the business dynamics changes rapidly,
and approaches that allow quick iteration of software, such as agile
development, are much more preferred \cite{agile.se}. However, even in the
academia, formal verification is not frequent, outside of critical areas. One
central reason could be that formalization of software is simply too
time-consuming \cite{formal.methods.underuse}; as a price of having reduced
software logic into primitive steps, much more of such steps are required.

That might seem to be an essential difficulty preventing the wide-adoption of
formal methods in software engineering, but another way of dealing with the
problem is increasing the formalization speed. One approach is to formally build
more advanced and verified steps out of primitive steps and define them in
\emph{proof assistants}. For example, Coq (Rocq) allows one to build custom
tactics and use a meta-language such as Ltac to manipulate tactics at a higher
level \cite{coq.manual}. Furthermore, LLMs, the current hot topic, have the
capacity to output seemingly correct formalization at high speed
\cite{llm.gen.proof}.  Combined with proof assistants, which can automatically
verify the correctness of the output, we could have the potential to greatly
speed up formalization of not only software, but also of informal mathematical
proofs found everywhere in mathematical texts, if the product of the correct
rate and the output speed of LLMs could surpass that of ours.

\subsubsection{How formalization may change collaboration}
One interesting application of proof assistants is Massot's \texttt{Blueprint}
\cite{massot.blueprint}. It is mostly a dependent graph that links each piece
(lemma) of a big proof as its nodes, which one can click on to go to its
rendered \LaTeX, \LaTeX\ source, and Lean (a proof assistant) source. Depending
on a node's formalization status, it's displayed differently in the graph
\cite{tao.blueprint.post}. Beside being used in various math projects (see its
README), it also has the potential to support building interactive math
textbooks for students.

In another direction, the deep trust that was previously required for each
collaborator's math skills, can now be reduced by automatic verification with
proof assistants. Thus, whereas a research-level math project used to be done
by a few professional mathematicians only, such one now can be instead
conducted this way: 1) the few expert lead mathematicians design a whole picture
of the theorem to prove and divide it into pieces 2) people, even those who are
not professional mathematicians, can claim pieces that they think they can
solve. 3) when they give solutions, it's the proof assistants' job to verify
them. As a proof of concept, Tao has led a pilot project conducted such way
\cite{tao.pilot.project}.

While most software engineering projects require far less trust than a
math project does, projects of engineering critical secure systems could
still demand a high-level of trust and thus benefit from proof assistants in
the same way as math projects above. In particular, could the development of a
fully-verified and quite complete (in the sense of containing drivers, file
systems, and other components, unlike a microkernel) OS kernel be possible, if
collaborated this way? That would be an interesting question to answer in my
future research.

\subsection[Free software, free society]{
Free software, free society\protect\footnotemark}
\footnotetext{The title here is the book title of the essays collection of
Dr.~Richard Stallman \cite{stallman.essays}.}

As our lives are more digitalized, digital computers are playing an
increasingly important role in them. Consequently, she who controls the
computers will have great power over our lives. But do we control the computers
(in the general sense, including mobile phones, smart devices, etc.) that we
purchased and own? By the very definition, software controls our computers.
Then, the question becomes, do we control the software that runs on our
computers?

Unfortunately, the answer is no for proprietary software. Stallman
has a number of nice essays on this topic \cite{stallman.essays}, and I would
just use a simple analogy here: using proprietary software is like entrusting
one's money to another person, whose operations are opaque, whose decisions one
has little or no influence on, whose interests are mainly profit, and who
otherwise has no connection with one. Clearly, no sane person would choose to
do that.  The core problem of proprietary software is that via them the
developers impose such an unjust power imbalance over its users, which tends to
corrupt and lead to mistreatment of users, as history has repeatedly shown us.

By launching the free software movement, Stallman sought to restore power to
users by securing their essential freedoms \cite[Essay~1]{stallman.essays}. The
movement has achieved remarkable successes, yet today it still faces serious
obstacles. An prominent example is the ideology of open source. Often
associated with Raymond’s The Cathedral and the Bazaar \cite{cathedral.bazaar},
open source in fact emerged from within the free software community as a
deviation from its core philosophy. As Stallman emphasizes, ``The term `open
source' quickly became associated with ideas and arguments based only on
practical values, such as making or having powerful, reliable software''
\cite[Essay~14]{stallman.essays}. By shifting focus from freedom to utility,
open source has obscured the very principles the free software movement was
founded to protect.

There are two significant problems of the current situation of open source. The
first is about the licenses. Some licenses only open the source code but
prevents unauthorized modification and sharing; some others grant total
access but fail to prevent the work from being stolen (i.e.\ creating a
proprietary fork from it). 

The second problem concerns the nature of freedom itself. Freedom is not
passive; it requires active exercise. Many users of free or open source
software almost never inspect, modify, or share the code
\cite{open.source.contrib}, thereby remaining as dependent on developers as
users of proprietary systems. Their situation differs only in that the
developers of free software may act more ethically and the part of the users
who do exercise their freedoms can somewhat prevent the developers from going
rogue. Yet the open source movement's deliberate avoidance of freedom as a
guiding value undermines awareness of it within the broader community. As
practical values dominate, free software communities risk being taken over by
open source members. Even worse are corporate-led open source projects such as
Microsoft’s VSCode: sustained by contributions from the whole community, but
mostly directed by company employees whose decisions reflect corporate
interests \cite{corporate.open.source, fake.open.source}. The result is
software that strengthens corporate power while exploiting users (e.g.\ VSCode
has built-in telemetry that is difficult to disable), building on top of
people's free (in terms of price) contribution. I see this as a profound yet
subtle undermining of the free software movement, and addressing this challenge
is essential as computing freedom is increasingly important in achieving a free
society.


\section[Mathematics]{Mathematics: My purer philosophical pursuits}
\label{sec.math}
As much as I am driven by real-life problems to understand systems better, the
force behind my sometimes unusally deep dive into even more primitive
principles is my strong desire to understand the foundational reasonings. In
this section, I will describe how that has led me to explore the purer
mathematical areas and my current interests in them.

\subsection{Type theory and metamathematics}
In the manuals of various proof assistant languages, technical terms from type
theory such as universe, sort, and introduction \& elimination rules, are
frequently mentioned. For me who is driven to understand what's going on
behind, contact with type theory is my destiny. Furthermore, I ventured deeper
into its history and the philosophical doctrines it was born from.

I have thought of a few ways to present this section, and in the end I decided
that I will present it as a narrative of the basic history. If you share my
curiosity for foundational reasonings, then after seeing the history I
presented in my style, I believe you will also see my passion.  A good starting
point is probably Cantor's work on his theory of abstract sets, particularly
his ingenious abstraction of sets to cardinal numbers for their comparsion.  In
1638 Galileo noted that there is a bijection between the natural numbers and
their squares: $f : n \mapsto n^2$, raising questions about the principle that
the whole is greater than any of its parts. Cantor took this further to
systematically compare sets, using bijections as the primary tool. 

An essential mental construct that Cantor used was his diagnoal argument, which
is often presented when proving the real numbers is not countable. Yet the
central philosophical reasoning behind this argument applies to general sets
which we don't know are countable or not, and this leads to this important
theorem, whose proof I present because it's short and raises questions about
many things.
\begin{CT}
There is no surjection from any set $S$ to the set of all of its subsets
$\mathcal{P}(S)$. In Cantor's words, the cardinal number of $\mathcal{P}(S)$ is
always larger than that of $S$.
\end{CT}
\begin{proof}
When $S$ is empty, it is trival that there's no surjection between $\emptyset$
and $\{\emptyset\}$. Then we consider when $S$ is not empty.

Suppose for now there exists a bijection $f : S \to
\mathcal{P}(S)$. $f$'s being a bijection enables us to apply Cantor's
diagonal argument, because now we can make sure the number of rows and columns
are the same: arrange such a matrix that each column corresponds to an element
$s \in S$, and each row is arranged to correspond to the sequence $f(s)$ for
all $s$, in the same order as the columns. Along the diagonal, flip each
element to define this subset $T \subset S$ such that $s \in T$ iff $s \notin
f(s)$. Although we used the bijection fact to get to this definition, the
definition itself does not require a $f$'s being a bijection; it can be any
function $f : S \to \mathcal{P}(S)$. But we will need it to be a surjection for
the argument that follows. So we now demote $f$ to be any surjection.

Because $f$ is a surjection, there is such $s_0$ that $f(s_0) = T$. Now we ask
if $s_0 \in T$. Note that here we implicitly assume either $s_0 \in T$ or $s_0
\notin T$. In either case, by definition of $T$, we will arrive at a
contradiction. Thus, by \emph{reductio ad absurdum}, it is not the case
that there exists such a surjection $f$.
\end{proof}

Cantor and Russell arrived at two similar paradoxes, both involving the set of
all sets.
\begin{description}
	\item[Cantor's paradox] Consider the set of all sets $\Omega$ and its
		power set $\mathcal{P}(\Omega)$. By Cantor's theorem, the latter should
		be larger than the former. On the other hand, $\mathcal{P}(\Omega)$,
		being a set of sets, should be a subset of $\Omega$, a contradiction.
	\item [Russell's paradox] When Russell was later analyzing Cantor's
		theorem, he wondered what if $S$ itself was $\Omega$. Under the
		assumption that all objects are sets, we thus have $S =
		\mathcal{P}(S)$, and the simple $f: s \mapsto s$ serves as a bijection
		between them. In that case, Cantor's diagnoal subset $T$ becomes the
		famous form that we know of: $T := \{s \mid s \notin s \}$.
\end{description}

For Russell, his paradox was discovered at a bad time, when he was on his way
of finishing his \emph{Principles of Mathematics}, and when he assumed that
there was a universal set. He had hoped for a single, unified solution to the
paradoxes of logic. But to his discouragement, after a few years of various
attempts to fix them in the way he hoped, he was eventually forced to come to
his ramified theory of types, using a hierachy of types to classify
propositions \cite{companion.to.russell.tt}.

Perhaps one reason why the paradoxes were so difficult to fix was that Cantor's
diagonal method had revealed something deeply wrong about the previous
mathematical reasoning. To avoid the paradoxes, one prominent work was
Zermelo's axiomatic set theory (later refined by others) which restricted what
sets can be. But there are still solved problems. Previously, mathematicians
thought that they were working with real and complete math objects such as sets
and numbers and that they were speaking truths of them as theorems. Since that
had lead to contradictions, there must have been something wrong in the
foundation of what we believed these objects to be or our logic. Even if we
discard the previous foundation entirely and adopt the axiomatic theory as the
new foundation, there must still be the matter of truth and meaning --- we
cannot adopt the axiomatic set theory entirely formally as the new foundation,
without a meaning about what we think is true or not \cite[Ch.~12]{kleeneitmm}.

\emph{Unfinished.}

\include{additional}

\bibliographystyle{acm}
\bibliography{main.bib}

\end{document}
