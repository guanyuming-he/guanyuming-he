\documentclass{article}

\usepackage{amsmath, amsthm, amssymb}
% See amsthdoc p.~3
\newtheorem*{CT}{Cantor's Theorem}

\usepackage{hyperref}

\begin{document}
\tableofcontents

\section{Security: Introduction into my interests}
I believe security is a good entrance into my current interests. It awards both
attention to detail and high-level systematic view; its subfields connects
directly with both theoretical and practical areas that fascinates me, and it
offers effective tools to solve the imminent problems I face in real life. 

The term \emph{security} in computer science generally carries the sense of
achieving \emph{security goals} through \emph{mechanisms or properties} of a
system, despite the presence of adversaries in a \emph{threat model}. The
threat model has to be carefully chosen and reviewed; there would be no
security against an omnipotent adversary.

Unlike in math, one cannot arbitrarily wiggle the threat model to one's desire.
On the contrary, one's rights are increasingly threatened by the
powerful digitally \cite{eu.digital.1, eu.digital.2, internet.shutdown.2024}. A
prominent example is the continuous push for exceptional access to to people's
data and communication in various forms by governments\footnote{ For instance,
	EU revived the \href{
https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=COM\%3A2022\%3A209\%3AFIN}
{Chat Control} proposal in 2025. See \url{
https://eutechloop.com/time-is-running-chat-control/}.}, ignoring expert
opinions against such measures \cite{keys.under.doormats, bugs.in.our.pockets,
chatcontrolchildprotection}.
I unfortunately has suffered from more aggressive attacks, primarily
systematic, comprehensive, and far-reaching censorship \cite{internet.coup}
\cite[Sect.~5]{chall.censor.circum}, because of my background.

How my security goal of being able to exercise my human rights in the presence of
these powerful adversaries drives my interests will be explained by the
following subsections, some of which also describes how my more pure academic
excitements are intertwined with this realistic drive. Finally, the last
subsection, \ref{sec.crypto}, will lead the transition into the other areas of
my interest.

\subsection{Secure Systems: Understanding the mechanisms} 
\label{sec.secure.systems}
\subsubsection{A high-level view of the Internet}
A high-level understanding of how the digital infrastructures work is essential
to understand what enables these threats. The current networking infrastructure,
in my opinion, has conflicting properties. On one hand, the fundamental problem
of the impossibility to link every two computers requires sharing of links, a
solution that welcomes centralization. On the other hand, the poor scalability
of simple sharing schemes of a link (usually via a switch) necessitates a
better scheme to extend a small network globally. To achieve this, the Internet
relies heavily on delegation and distributed structures, such as topological
divisions in its address and delegation of most routing responsibilities to
each individual networks (e.g.\ ISPs). Its BGP, opearing in between them, is
mainly concerned exchanging reachability information, whereas its IGPs handle
routing paths and allow different networks to implement different routing
policies.

Therefore, the Internet has become a mixture of centralization and
decentralization, where each end node is managed by an ISP network, yet no
single ISP runs the whole Internet. Perhaps surprisingly, I found this hybrid
structure more optimal for localized sabotage at the state level, creating
``sub-Internet''s, each of which is crippled at a different level. 

Unfortunately, the lower layers of the Internet have proven to have a great
inertia for change.  Handley gave a nice discussion on it in 2006
\cite{why.internet.just.works}, and the trend he described has mostly been the
same since then: ``\emph{the core Internet protocols have not changed
significantly in more than a decade, in spite of exponential growth in the
number of Internet users and the speed of the fastest links.}''
\cite{why.internet.just.works}. On the other hand, the protocols in the higher
layers evolved to a much greater degree. As the transport layer welcomes
QUIC\cite{quic}, the application layer has accumulated an enormous amount of
innovation and progress, in particular, onion routing and
Tor\cite{onion.routing, tor}, Bitcoin\cite{bitcoin}, VPN
protocols\cite{openvpn, wireguard} and decentralized instant chat\cite{matrix,
tox}, that fight for digital rights and/or promote decentralization. Yet one
must not overlook the crypto-constructs that lay the foundation for all of
them, which I discuss in detail in Section~\ref{sec.crypto}.

Unfortunately again, because all of these are built on the Internet, a
state-level censor could easily abuse its local authority to target them. A few
regimes are notorious to have blocked a vast amount of them to various
extents, employing complicated passive analysis and active probing techniques
\cite{censor.block.1, censor.block.2,censor.block.3, censor.block.4,
censor.block.5,censor.block.6},
Although a state would need to consider the collatoral damage, a totalitarian
regime would not hesitate to block an entire protocol before it can figure out
how to block it selectively \cite{selective.block.1, censor.block.4}. 
Apart from state-level actors, because commerical local ISPs additionally
have an incentive in income and profit, not only do they perform surveillance
and censorship like state-actors \cite{isp.statelike.actions.1,
isp.statelike.actions.2}, they also implement unjust policies easily with their
local control of the infrastructure, like unfairly limiting the use of certain
P2P protocols \cite{isp.block.p2p.1, isp.block.p2p.2, isp.block.p2p.3,
isp.statelike.actions.1}.  Although ISPs argue that these P2P protocols can
consume too much bandwidth, the other side of the story is that ISPs often
oversubscribe and fraudulently advertise the bandwidth of Internet service they
provide \cite{isp.oversubscribe.1, isp.oversubscribe.2}. This essentially is a
probabilistic exploit on its customers --- when almost all of the users happen
to use the maximal bandwidth the ISP sells to them, cogestion and throttling
occur, and the users, not the ISP, ultimately pay the price --- this is also
the same kind of injustice imposed by airlines who oversell tickets.

The question of how to build protocols and systems that preserves one's rights
on top of the Internet that powerful adversaries control, fighting against
surveillance, censorship, and overall other unfair practices which the
infrastructures of the current networks happen to enable, is central to my
interest in secure distributed systems.

\subsubsection{Low-level system details}
Beside the threat model, when we talk about security, we also implicitly assume
another model, environment, or host, that encapsulates the problem. For
instance, in the context of isolation, 
\begin{itemize}
	\item The host that encapsulates process isolation is the operating system
		kernel.
	\item The host that encapsulates virtual machine isolation is the
		hypervisor, often including hardware support.
	\item The host that encapsulates air-gapped machine isolation is the
		physical world, or more abstractly, the physical laws.
\end{itemize} 
These hosts form a hierarchy. For instance, processes inside an OS may run
inside a virtual machine, on hardware, within the physical world.

As a result, a security researcher has to understand these hosts, each to a
different degree, depending on how secure she wants her systems to be. Even if
a system is formally verified to be secure within a host, it can still be
attacked from interactions with an outer host. That is well examplified by
the Meltdown attack, which breaks both kernel and hypervisor's isolation of
memory, because an outer host, the hardware, has a vulnerability
\cite{meltdown}. In fact, Meltdown touches more than logical flaws in hardware;
it relies on the timing difference of cache accesses to extract information
from it \cite{flush.reload}, one that directly connects with the physical
world. At this point, I feel that I should also mention the Spectre attacks
family \cite{spectre}, which, unlike Meltdown, does not exploit a clear logic
bug or fault in the hardware. Instead, they work by observing the side effects
of various implementations of speculative execution, which is arguably not a
bug but simply a feature implemented without caution to how information may be
leaked via the outer hosts.

Indeed, there is little hope to understand and model the universe thoroughly
and accurately in one's life-time, and one can expect endless discoveries of
how an otherwise secure system can leak information in unexpected ways, such as
power and radiation via physical effects, which opens to a large range of
attacks \cite{side.channel.1, side.channel.2, side.channel.3, flush.reload},
collectively defined as side-channel attacks.  Then, one must ask, is it futile
trying to cope with every such possible attack? My opinion is that it depends
on the current understanding of the whole cybersecurity community. One novel
attack just discovered may be increasingly understood and gain popularity by
the whole community and thus gradually become a necessary part every secure
system designer should take into account. I view this as an everlasting
dynamics where people continuously push the boundary of our understanding, and
consequently expanding the requirements for all in the community.

Therefore, a good understanding of the most common and well-understood hosts,
the operating systems and the hardware (including the architecture), will be
essential for me as a security researcher. Additionally, figuring out how to
perform clever hacks (as in the hacking culture of MIT, not the mainstream
meaning of cracking) based on knowledge of details of a system gives a great
sense of achievement to me. Moreover, this continuous expansion of knowledge
frontier carries a special implication in security. Whereas a mathematician is
perfectly fine to specialize in one direction without knowing every details in
other fields, a computer system can never be secure if only designed with one
perspective. For now, a good system is usually designed by collaboration; and
it will be a very interesting challenge to figure out how one single researcher
can be empowered to do this task better; could the LLMs be a potential
assistant who fills the knowledge gaps? Finally, this is one major place where
cybersecurity connects directly with a broad range of other fields, including
software engineering, systems architecture, networking, and physics.  These
together contribute to my interest in the low-level details of secure systems.

\subsection{Data-driven analysis: Lights through artifical clouds}
Despite my intention to understand systems, many of them are not open for
analysis, for various reasons. In particular, the aforementioned systems for
digital surveillance and censorship are not only proprietary but often state
secrets. On the other hand, similar to side channel attacks, these systems
inevitably leak information in different ways.

By carefully observing their behavior and probing their responses under
controlled conditions, we can gain insight. Combined with data science
techniques, these observations reveal hidden structures and enable us to draw
meaningful conclusions.

The process usually involves 1) raising questions and claims about some
properties of a target system 2) conducting experiments (often involving
purchased servers within such as Aliyun) 3) analyzing data 4) answering
questions and verifying claims. As a simple example, Sheffey et al.\ very
recently studied the IP addresses injected by the GFW censorship system. By
first finding injected IPs and then probing them within the GFW, they found
three categories of such IPs \cite{gfw.injected.ip}. 

Thanks to continuous work from both academic scholars and dedicated
organizations, (e.g.\ \cite{data.analysis.1, data.analysis.2, censor.block.6}),
I and a small set of others who suffer from such systems and who are fortunate
enough to know these works, could know what we are facing everyday better.
Conversely, I strongly hope to contribute to the free-side of the arms race,
making the free Internet reachable to everyone.

\subsection{Cryptography: A mathematical savior} \label{sec.crypto}
Security is often described as an arms race --- that who controls more
resources tends to discover more vulnerabilities, devise more attacks, and
harden their systems more. Where is hope, then, when most self-censor and give
up fighting the regime \cite{self.censor.1, self.censor.2, self.censor.3}  and
even fewer of the fighters have sufficient skills to join in the crypto war
\cite{defenders.lack.skills.1, defenders.lack.skills.2}?

I believe one solution to what might sound like a power struggle lies within
(modern) cryptography. For thousands of years since the use of the earliest
symmetric encryption methods like the Ceasar cipher, the ability to securely
communicate through an insecure channel had still been mostly limited to the
privileged who could afford persistent access to physical secure channels to
exchange the keys and reliable safeguarding of the keys.  A stunning turning
point was found in what was widely regarded as the beginning of modern
cryptography, Deffie and Hellman's \emph{New Directions in Cryptography}
\cite{new.directions.crypto}, where they gave a practical mathematical
procedure to Merkle's original idea for establishing a key known only to both
parties over an insecure channel.

At first glance, the idea sounded so impossible that Merkel himself faced 
rejections when presenting the idea to his then professor Hoffman and to the
CACM \cite{merkle.rejection}:
\begin{quotation}
	``I am sorry to have to inform you that the paper is not in the main stream
	of present cryptography thinking and I would not recommend that it be
	published in the Communications of the ACM.''

	``Experience shows that it is extremely dangerous to transmit key
	information in the clear.''\cite{merkle.rejection}
\end{quotation}
The rejections represented a concensus of the old cryptography community that
even Shannon concurred with: ``\emph{The key must be transmitted by
non-interceptible means from transmitting to receiving points}''
\cite[p.~670]{shannon.theory.secrecy}, which demonstrates how ``paradoxical''
the idea was.

Such ``paradoxical'' ideas of modern cryptography are what attract me most to
it, because they are the enabler that underlies those systems mentioned in
section~\ref{sec.secure.systems}: TOR, Bitcoin, OpenVPN, Matrix, etc., and the
catalyst that leads to the massive adoption of society advancements such as
e-commerce. Remarkably, in less than 50 years since 1976, we already have a
number of such discoveries in addition, and here's a list of some of them.
\begin{description}
\item[Pseudorandom functions] The kind of deterministic algorithms
	whose outputs look like that of a random oracle, by block ciphers like
	AES\cite{aes} or other ways \cite{pseudo.rand.cons.2}, has a strong link
	with various other essential constructs like one-way functions.

\item[Zero-knowledge proofs] By interactively leveraging challenges that only a
	true prover could easily solve, zero-knowledge proof \cite{zero.knowledge}
	enables one to verify that the prover knows a witness of a problem in NP
	\cite{zero.knowledge.np} without revealing the witness.

\item[Homomorphic encryptions] Doing computation on encrypted data has been a
	very desirable property for many years. Although many earliest public-key
	encryption schemes, such as RSA \cite{rsa} and ElGamal \cite{elgamal},
	already natively supported limited homomorphism like modular
	multiplication, by their design, the first full scheme that allowed
	arbitrary computation on encrypted data was only proposed in 2009
	\cite{first.full.homo}.
\end{description}

These discoveries have many applications and implications, two of which I pay
most attention to. One is that they update our understanding of certain
theoretical lower bounds of how much security one can achieve in the face of
strong adversaries, no matter the power difference between them. The other is
how they help minimize the trust required to perform global-level of
collaboration that involves people from vastly different backgrounds and
beliefs; such a collaboration that would typically require a strong central
authority to organize, now only requires the participate to trust a few
foundamental assumptions of cryptography.

Personally, I would go a step further. I believe modern cryptography offers not
only technical tools, but a possible solution for addressing one of the big
challenges our democracies face today: fragmentation, polarization, and the
erosion of shared trust. By design, cryptography redistributes power --- it
makes mass surveillance prohibitively costly, even for state-level actors, by
forcing them to target individuals rather than populations. For example,
suppose breaking one personâ€™s encryption, on average, required a single day of
dedicated effort (through side-channels, vulnerabilities, or social engineering
rather than mathematics), that constraint alone would prevent massive
surveillance, since no adversary has that many days or resources to do this on
everyone. In this way, encryption does something profound: it automatically and
passively unites individuals, protecting each not by active coordination that
is increaingly difficult, but by the collective shield of widespread adoption.
Fortunately, as many modern infrastructures already deploy encryption and other
crypto schemes by default, this ``passive solidarity'' is not a dream but a
reachable reality, one that shows how mathematics can serve as a safeguard for
democracy in an increasingly divided world.

To end this section, cryptography not only flows towards my passion for purer
philosophical understanding, because of its deep connection with theoretical
computer science and mathematics, to which I turn in Section~\ref{sec.math},
but also carries my hope to break free from the strong grasp of the tyrannies
today, one that represents my free will which refuses to lose my agency and
independence, in a world that too often seeks to reduce us into interchangable
screws for the system.  

\section{Mathematics: My purer philosophical pursuits} \label{sec.math}
As much as I am driven by real-life problems to understand systems better, the
force behind my sometimes unusally deep dive into even more primitive
principles is my strong desire to understand the foundational reasonings. In
this section, I will describe how that has led me to explore the purer
mathematical areas and my current interests in them.

\subsection{Type theory and metamathematics}
In the manuals of various proof assistant languages, technical terms from type
theory such as universe, sort, and introduction \& elimination rules, are
frequently mentioned. For me who is driven to understand what's going on
behind, contact with type theory is my destiny. Furthermore, I ventured deeper
into its history and the philosophical doctrines it was born from.

I have thought of a few ways to present this section, and in the end I decided
that I will present it as a narrative of the basic history. If you share my
curiosity for foundational reasonings, then after seeing the history I
presented in my style, I believe you will also see my passion.  A good starting
point is probably Cantor's work on his theory of abstract sets, particularly
his ingenious abstraction of sets to cardinal numbers for their comparsion.  In
1638 Galileo noted that there is a bijection between the natural numbers and
their squares: $f : n \mapsto n^2$, raising questions about the principle that
the whole is greater than any of its parts. Cantor took this further to
systematically compare sets, using bijections as the primary tool. 

An essential mental construct that Cantor used was his diagnoal argument, which
is often presented when proving the real numbers is not countable. Yet the
central philosophical reasoning behind this argument applies to general sets
which we don't know are countable or not, and this leads to this important
theorem, whose proof I present because it's short and raises questions about
many things.
\begin{CT}
There is no surjection from any set $S$ to the set of all of its subsets
$\mathcal{P}(S)$. In Cantor's words, the cardinal number of $\mathcal{P}(S)$ is
always larger than that of $S$.
\end{CT}
\begin{proof}
When $S$ is empty, it is trival that there's no surjection between $\emptyset$
and $\{\emptyset\}$. Then we consider when $S$ is not empty.

Suppose for now there exists a bijection $f : S \to
\mathcal{P}(S)$. $f$'s being a bijection enables us to apply Cantor's
diagonal argument, because now we can make sure the number of rows and columns
are the same: arrange such a matrix that each column corresponds to an element
$s \in S$, and each row is arranged to correspond to the sequence $f(s)$ for
all $s$, in the same order as the columns. Along the diagonal, flip each
element to define this subset $T \subset S$ such that $s \in T$ iff $s \notin
f(s)$. Although we used the bijection fact to get to this definition, the
definition itself does not require a $f$'s being a bijection; it can be any
function $f : S \to \mathcal{P}(S)$. But we will need it to be a surjection for
the argument that follows. So we now demote $f$ to be any surjection.

Because $f$ is a surjection, there is such $s_0$ that $f(s_0) = T$. Now we ask
if $s_0 \in T$. Note that here we implicitly assume either $s_0 \in T$ or $s_0
\notin T$. In either case, by definition of $T$, we will arrive at a
contradiction. Thus, by \emph{reductio ad absurdum}, it is not the case
that there exists such a surjection $f$.
\end{proof}

Cantor and Russell arrived at two similar paradoxes, both involving the set of
all sets.
\begin{description}
	\item[Cantor's paradox] Consider the set of all sets $\Omega$ and its
		power set $\mathcal{P}(\Omega)$. By Cantor's theorem, the latter should
		be larger than the former. On the other hand, $\mathcal{P}(\Omega)$,
		being a set of sets, should be a subset of $\Omega$, a contradiction.
	\item [Russell's paradox] When Russell was later analyzing Cantor's
		theorem, he wondered what if $S$ itself was $\Omega$. Under the
		assumption that all objects are sets, we thus have $S =
		\mathcal{P}(S)$, and the simple $f: s \mapsto s$ serves as a bijection
		between them. In that case, Cantor's diagnoal subset $T$ becomes the
		famous form that we know of: $T := \{s \mid s \notin s \}$.
\end{description}

For Russell, his paradox was discovered at a bad time, when he was on his way
of finishing his \emph{Principles of Mathematics}, and when he assumed that
there was a universal set. He had hoped for a single, unified solution to the
paradoxes of logic. But to his discouragement, after a few years of various
attempts to fix them in the way he hoped, he was eventually forced to come to
his ramified theory of types, using a hierachy of types to classify
propositions \cite{companion.to.russell.tt}.

Perhaps one reason why the paradoxes were so difficult to fix was that Cantor's
diagonal method had revealed something deeply wrong about the previous
mathematical reasoning. To avoid the paradoxes, one prominent work was
Zermelo's axiomatic set theory (later refined by others) which restricted what
sets can be. But there are still solved problems. Previously, mathematicians
thought that they were working with real and complete math objects such as sets
and numbers and that they were speaking truths of them as theorems. Since that
had lead to contradictions, there must have been something wrong in the
foundation of what we believed these objects to be or our logic. Even if we
discard the previous foundation entirely and adopt the axiomatic theory as the
new foundation, there must still be the matter of truth and meaning --- we
cannot adopt the axiomatic set theory entirely formally as the new foundation,
without a meaning about what we think is true or not \cite{kleeneitmm}.

\bibliographystyle{acm}
\bibliography{main.bib}

\end{document}
